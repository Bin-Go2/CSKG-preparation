{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84163f3ca19c0b7c9fda47121b3bc4cadfaf1fcc"
   },
   "source": [
    "# Gensim Word2VecÂ Tutorial\n",
    "reference: https://www.kaggle.com/sonders/gensim-word2vec-tutorial/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa10f4823faa729e96f15bb865e9f20769ea7a4b"
   },
   "source": [
    "# Plan\n",
    "\n",
    "1. [Briefing about Word2Vec](#Briefing-about-Word2Vec:)\n",
    "    * [Purpose of the tutorial](#Purpose-of-the-tutorial:)\n",
    "    * [Brief explanation](#Brief-explanation:)\n",
    "\n",
    "2. [Getting Started](#Getting-Started)\n",
    "    * [Setting up the environment](#Setting-up-the-environment:)\n",
    "    * [The data](#The-data:)\n",
    "3. [Preprocessing](#Preprocessing)\n",
    "    * [Cleaning](#Cleaning)\n",
    "    * [Bigrams](#Bigrams)\n",
    "    * [Most frequent words](#Most-Frequent-Words)\n",
    "    \n",
    "4. [Training the Model](#Training-the-model)\n",
    "    * [Gensim Word2Vec Implementation](#Gensim-Word2Vec-Implementation:)\n",
    "    * [Why I seperate the training of the model in 3 steps](#Why-I-seperate-the-training-of-the-model-in-3-steps:)\n",
    "    * [Training the model](#Training-the-model)\n",
    "        * [The parameters](#The-parameters)\n",
    "        * [Building the vocabulary table](#Building-the-Vocabulary-Table)\n",
    "        * [Training of the model](#Training-of-the-model)\n",
    "        * [Saving the model](#Saving-the-model:)\n",
    "5. [Exploring the Model](#Exploring-the-model)\n",
    "    * [Most similar to](#Most-similar-to:)\n",
    "    * [Similarities](#Similarities:)\n",
    "    * [Odd-one-out](#Odd-One-Out:)\n",
    "    * [Analogy difference](#Analogy-difference:)\n",
    "    * [t-SNE visualizations](#t-SNE-visualizations:)\n",
    "        * [10 Most similar words vs. 8 Random words](#10-Most-similar-words-vs.-8-Random-words:)\n",
    "        * [10 Most similar words vs. 10 Most dissimilar](#10-Most-similar-words-vs.-10-Most-dissimilar:)\n",
    "        * [10 Most similar words vs. 11th to 20th Most similar words](#10-Most-similar-words-vs.-11th-to-20th-Most-similar-words:)\n",
    "6. [Final Thoughts](#Final-Thoughts)\n",
    "7. [Material for more in depths understanding](#Material-for-more-in-depths-understanding:)\n",
    "8. [Acknowledgements](#Acknowledgements)\n",
    "9. [References](#References:)\n",
    "10. [End](#End)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d96105f0c90bf052b2afdb684bf31549e1e6c81"
   },
   "source": [
    "# Briefing about Word2Vec:\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" alt=\"drawing\" width=\"550\"/>\n",
    "\n",
    "\n",
    "## Purpose of the tutorial:\n",
    "As I said before, this tutorial focuses on the right use of the Word2Vec package from the Gensim libray; therefore, I am not going to explain the concepts and ideas behind Word2Vec here. I am simply going to give a very brief explanation, and provide you with links to good, in depth tutorials.\n",
    "\n",
    "## Brief explanation:\n",
    "\n",
    "Word2Vec was introduced in two [papers](#Material-for-more-in-depths-understanding:) between September and October 2013, by a team of researchers at Google. Along with the papers, the researchers published their implementation in C. The Python implementation was done soon after the 1st paper, by [Gensim](https://radimrehurek.com/gensim/index.html). \n",
    "\n",
    "The underlying assumption of Word2Vec is that two words sharing similar contexts also share a similar meaning and consequently a similar vector representation from the model. For instance: \"dog\", \"puppy\" and \"pup\" are often used in similar situations, with similar surrounding words like \"good\", \"fluffy\" or \"cute\", and according to Word2Vec they will therefore share a similar vector representation.<br>\n",
    "\n",
    "From this assumption, Word2Vec can be used to find out the relations between words in a dataset, compute the similarity between them, or use the vector representation of those words as input for other applications such as text classification or clustering.\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "## Setting up the environment:\n",
    "\n",
    "`python==3.6.3`\n",
    "\n",
    "Libraries used:\n",
    " * `xlrd==1.1.0`: https://pypi.org/project/xlrd/\n",
    " * `spaCy==2.0.12`: https://spacy.io/usage/\n",
    " * `gensim==3.4.0`: https://radimrehurek.com/gensim/install.html\n",
    " * `scikit-learn==0.19.1`: http://scikit-learn.org/stable/install.html\n",
    " * `seaborn==0.8`: https://seaborn.pydata.org/installing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "cc7b3e6ca62670ff13626705402f626778487204"
   },
   "outputs": [],
   "source": [
    "import re                            # For preprocessing\n",
    "import pandas as pd                  # For data handling\n",
    "from time import time                # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import spacy                         # For preprocessing\n",
    "import logging                       # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ec7ce784f6d9e7f71e2b5789b1e65ec4414628b"
   },
   "source": [
    "## The data:\n",
    "I chose to play with the script from the Simpsons, both because I love the Simpsons and because with more than 150k lines of dialogues, the dataset was substantial!\n",
    "\n",
    "This dataset contains the characters, locations, episode details, and script lines for approximately 600 Simpsons episodes, dating back to 1989. It can be found here: https://www.kaggle.com/ambarish/fun-in-text-mining-with-simpsons/data (~25MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c36323d9aa62f74ab348cda5ee0f571aa1d4a96"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "We keep only two columns:\n",
    "* `raw_character_text`: the character who speaks (can be useful when monitoring the preprocessing steps)\n",
    "* `spoken_words`: the raw text from the line of dialogue\n",
    "\n",
    "We do not keep `normalized_text` because we want to do our own preprocessing.\n",
    "\n",
    "You can find the resulting file here: https://www.kaggle.com/pierremegret/dialogue-lines-of-the-simpsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6453b9c3f797e51923e030090ead659253f4e459"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158314, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('simpsons_dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "c6c6bf4462fb4bc00c2abdbf65eced888219f364"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "caf6838463d56f79e92d87d5a3827fcd5f04fc54"
   },
   "source": [
    "The missing values comes from the part of the script where something happens, but with no dialogue. For instance \"(Springfield Elementary School: EXT. ELEMENTARY - SCHOOL PLAYGROUND - AFTERNOON)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "3a15727caeba1c8d10573456640d0b8b9f2f2e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "885a555596d7484841ea54c94405d03d90572396"
   },
   "source": [
    "Removing the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "82cb38f176526679f66ee31e11cfe4f5eebdab51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:14:53: NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "raw_character_text    0\n",
       "spoken_words          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna().reset_index(drop=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f07dca2a2656dcd9e0c315afa36af32a992eef7"
   },
   "source": [
    "## Cleaning:\n",
    "We are lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "b26a0c01c5701630d3951cfc808a9d944eea6371"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f686964722eede40e5961cd232aee7b6dd587bd1"
   },
   "source": [
    "Removes non-alphabetic characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b45598934171607242ca7d50f8c5f7c91411aace"
   },
   "outputs": [],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2360160a7f326a32d56f2f18782d7ce2f4ac1def"
   },
   "source": [
    "Taking advantage of spaCy .pipe() attribute to speed-up the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "fa44ca458c970ca229426779e6ffcd46c2de313c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 1.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65ec76c60f9fe93bba6909f4e696f90e3e54710b"
   },
   "source": [
    "Put the results in a DataFrame to remove missing values and duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "57f1eb8382554bc592d48915a903230b5b6d6cf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85954, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "31b4a744059df490ddb47ab6cdec008dc929ede3"
   },
   "source": [
    "## Bigrams:\n",
    "We are using Gensim Phrases package to automatically detect common phrases (bigrams) from a list of sentences.\n",
    "https://radimrehurek.com/gensim/models/phrases.html\n",
    "\n",
    "The main reason we do this is to catch words like \"mr_burns\" or \"bart_simpson\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "af6d420284a0ff7a7407d4c526754ffe850d6170"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "788aec3c82788101db25d4ca6105ee133fecae7c"
   },
   "source": [
    "As `Phrases()` takes a list of list of words as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "f58487ff08d8812622fd7aef36139f1c850add18"
   },
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb7766b322cbc1d3381912b890585eb249ac5304"
   },
   "source": [
    "Creates the relevant phrases from the list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "8befad8c76c54bd2b831b0942a2f626f7d8a6dac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:17:59: collecting all words and their counts\n",
      "INFO - 23:17:59: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:17:59: PROGRESS: at sentence #10000, processed 63557 words and 52796 word types\n",
      "INFO - 23:17:59: PROGRESS: at sentence #20000, processed 130938 words and 99801 word types\n",
      "INFO - 23:18:00: PROGRESS: at sentence #30000, processed 192959 words and 138413 word types\n",
      "INFO - 23:18:00: PROGRESS: at sentence #40000, processed 249832 words and 172509 word types\n",
      "INFO - 23:18:00: PROGRESS: at sentence #50000, processed 311271 words and 208406 word types\n",
      "INFO - 23:18:00: PROGRESS: at sentence #60000, processed 373576 words and 243519 word types\n",
      "INFO - 23:18:00: PROGRESS: at sentence #70000, processed 436427 words and 278547 word types\n",
      "INFO - 23:18:00: PROGRESS: at sentence #80000, processed 497891 words and 311704 word types\n",
      "INFO - 23:18:00: collected 330480 word types from a corpus of 537095 words (unigram + bigrams) and 85954 sentences\n",
      "INFO - 23:18:00: using 330480 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "45bae4a953f2ad8951e4efb234e1e357857a33b3"
   },
   "source": [
    "The goal of Phraser() is to cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b8ae81ba230013aefe7c584338de7376fedf6294"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:18:09: source_vocab length 330480\n",
      "INFO - 23:18:13: Phraser built with 127 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a58380f19d159688aeee665d1afb96289fdd4b8"
   },
   "source": [
    "Transform the corpus based on the bigrams detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "8051b56890c147119db3df529d3cfd3cf675fdca"
   },
   "outputs": [],
   "source": [
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4f81e8bb2c09a67b00cd24db28353eca8ae188c"
   },
   "source": [
    "## Most Frequent Words:\n",
    "Mainly a sanity check of the effectiveness of the lemmatization, removal of stopwords, and addition of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "eeb8afe1cfcb7ba65bd14d657455600acacf39ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30242"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "5b010149150b2b2eaf332d79bcde0649b8a3c2b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'like', 'know', 'get', 'hey', 'think', 'come', 'right', 'look', 'want']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "500ab7b5c84dc006d7945f339c40725a82856fdf"
   },
   "source": [
    "# Training the model\n",
    "## Gensim Word2Vec Implementation:\n",
    "We use Gensim implementation of word2vec: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "3269be205cadbad499aa87890893d92da6adc796"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c524bc49c41a6c37f9e754a38797c9501202090"
   },
   "source": [
    "## Why I seperate the training of the model in 3 steps:\n",
    "I prefer to separate the training in 3 distinctive steps for clarity and monitoring.\n",
    "1. `Word2Vec()`: \n",
    ">In this first step, I set up the parameters of the model one-by-one. <br>I do not supply the parameter `sentences`, and therefore leave the model uninitialized, purposefully.\n",
    "2. `.build_vocab()`: \n",
    ">Here it builds the vocabulary from a sequence of sentences and thus initialized the model. <br>With the loggings, I can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. I noticed that these two parameters, and in particular `sample`, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n",
    "3. `.train()`:\n",
    ">Finally, trains the model.<br>\n",
    "The loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "03488d9b68963579c96094aca88a302c9f2753a7"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89c305fcd163488441ac2ac6133678bd973b4419"
   },
   "source": [
    "## The parameters:\n",
    "\n",
    "* `min_count` <font color='purple'>=</font> <font color='green'>int</font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "\n",
    "\n",
    "* `window` <font color='purple'>=</font> <font color='green'>int</font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n",
    "\n",
    "\n",
    "* `size` <font color='purple'>=</font> <font color='green'>int</font> - Dimensionality of the feature vectors. - (50, 300)\n",
    "\n",
    "\n",
    "* `sample` <font color='purple'>=</font> <font color='green'>float</font> - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial.  - (0, 1e-5)\n",
    "\n",
    "\n",
    "* `alpha` <font color='purple'>=</font> <font color='green'>float</font> - The initial learning rate - (0.01, 0.05)\n",
    "\n",
    "\n",
    "* `min_alpha` <font color='purple'>=</font> <font color='green'>float</font> - Learning rate will linearly drop to `min_alpha` as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "\n",
    "\n",
    "* `negative` <font color='purple'>=</font> <font color='green'>int</font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "\n",
    "\n",
    "* `workers` <font color='purple'>=</font> <font color='green'>int</font> - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "ad619db82c219d6cb81fad516563feb0c4d474cd"
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7e9f1bd338f9e15647b5209ffd8fbb131cd7ee5"
   },
   "source": [
    "## Building the Vocabulary Table:\n",
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "66358ad743e05e17dfbed3899af9c41056143daa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:21:22: collecting all words and their counts\n",
      "INFO - 23:21:22: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:21:22: PROGRESS: at sentence #10000, processed 61710 words, keeping 9572 word types\n",
      "INFO - 23:21:22: PROGRESS: at sentence #20000, processed 127345 words, keeping 14535 word types\n",
      "INFO - 23:21:23: PROGRESS: at sentence #30000, processed 187806 words, keeping 17660 word types\n",
      "INFO - 23:21:23: PROGRESS: at sentence #40000, processed 243314 words, keeping 20424 word types\n",
      "INFO - 23:21:23: PROGRESS: at sentence #50000, processed 303176 words, keeping 22934 word types\n",
      "INFO - 23:21:23: PROGRESS: at sentence #60000, processed 363916 words, keeping 25246 word types\n",
      "INFO - 23:21:24: PROGRESS: at sentence #70000, processed 425379 words, keeping 27467 word types\n",
      "INFO - 23:21:24: PROGRESS: at sentence #80000, processed 485507 words, keeping 29350 word types\n",
      "INFO - 23:21:24: collected 30242 word types from a corpus of 523616 raw words and 85954 sentences\n",
      "INFO - 23:21:24: Loading a fresh vocabulary\n",
      "INFO - 23:21:24: effective_min_count=20 retains 3309 unique words (10% of original 30242, drops 26933)\n",
      "INFO - 23:21:24: effective_min_count=20 leaves 437086 word corpus (83% of original 523616, drops 86530)\n",
      "INFO - 23:21:24: deleting the raw counts dictionary of 30242 items\n",
      "INFO - 23:21:24: sample=6e-05 downsamples 1199 most-common words\n",
      "INFO - 23:21:24: downsampling leaves estimated 198624 word corpus (45.4% of prior 437086)\n",
      "INFO - 23:21:24: estimated required memory for 3309 words and 300 dimensions: 9596100 bytes\n",
      "INFO - 23:21:24: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.05 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63260d82061abb47db7f2f8b23e07ec629adf5a9"
   },
   "source": [
    "## Training of the model:\n",
    "_Parameters of the training:_\n",
    "* `total_examples` <font color='purple'>=</font> <font color='green'>int</font> - Count of sentences;\n",
    "* `epochs` <font color='purple'>=</font> <font color='green'>int</font> - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "07a2a047e701e512fd758edff186daadaeea6461",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:21:30: training model with 7 workers on 3309 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2\n",
      "INFO - 23:21:31: EPOCH 1 - PROGRESS: at 43.69% examples, 86233 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:32: EPOCH 1 - PROGRESS: at 86.15% examples, 83899 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:33: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:33: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:33: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:33: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:33: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:33: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:33: EPOCH - 1 : training on 523616 raw words (198345 effective words) took 2.3s, 85734 effective words/s\n",
      "INFO - 23:21:34: EPOCH 2 - PROGRESS: at 45.83% examples, 88753 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:35: EPOCH 2 - PROGRESS: at 93.83% examples, 89837 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:35: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:35: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:35: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:35: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:35: EPOCH - 2 : training on 523616 raw words (198260 effective words) took 2.2s, 91089 effective words/s\n",
      "INFO - 23:21:36: EPOCH 3 - PROGRESS: at 39.39% examples, 77509 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:37: EPOCH 3 - PROGRESS: at 86.15% examples, 83282 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:37: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:37: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:37: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:37: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:37: EPOCH - 3 : training on 523616 raw words (198396 effective words) took 2.3s, 84910 effective words/s\n",
      "INFO - 23:21:38: EPOCH 4 - PROGRESS: at 45.83% examples, 89110 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:39: EPOCH 4 - PROGRESS: at 88.13% examples, 85754 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:40: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:40: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:40: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:40: EPOCH - 4 : training on 523616 raw words (198528 effective words) took 2.3s, 87330 effective words/s\n",
      "INFO - 23:21:41: EPOCH 5 - PROGRESS: at 43.69% examples, 83707 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:42: EPOCH 5 - PROGRESS: at 88.13% examples, 81927 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:42: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:42: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:42: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:42: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:42: EPOCH - 5 : training on 523616 raw words (198719 effective words) took 2.4s, 83774 effective words/s\n",
      "INFO - 23:21:43: EPOCH 6 - PROGRESS: at 45.83% examples, 88167 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:44: EPOCH 6 - PROGRESS: at 91.99% examples, 88144 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:44: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:44: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:44: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:44: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:44: EPOCH - 6 : training on 523616 raw words (198322 effective words) took 2.2s, 88506 effective words/s\n",
      "INFO - 23:21:45: EPOCH 7 - PROGRESS: at 37.41% examples, 75282 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:46: EPOCH 7 - PROGRESS: at 82.30% examples, 81330 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:47: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:47: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:47: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:47: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:47: EPOCH - 7 : training on 523616 raw words (198946 effective words) took 2.4s, 82823 effective words/s\n",
      "INFO - 23:21:48: EPOCH 8 - PROGRESS: at 43.69% examples, 83299 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:49: EPOCH 8 - PROGRESS: at 84.22% examples, 81437 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:49: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:49: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:49: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:49: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:49: EPOCH - 8 : training on 523616 raw words (198713 effective words) took 2.4s, 82803 effective words/s\n",
      "INFO - 23:21:50: EPOCH 9 - PROGRESS: at 43.69% examples, 84865 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:21:51: EPOCH 9 - PROGRESS: at 90.04% examples, 87230 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:51: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:51: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:51: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:51: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:51: EPOCH - 9 : training on 523616 raw words (198777 effective words) took 2.4s, 83722 effective words/s\n",
      "INFO - 23:21:52: EPOCH 10 - PROGRESS: at 43.69% examples, 86461 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:53: EPOCH 10 - PROGRESS: at 90.04% examples, 88916 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:54: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:54: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:54: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:54: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:54: worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:21:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:54: EPOCH - 10 : training on 523616 raw words (198531 effective words) took 2.2s, 89374 effective words/s\n",
      "INFO - 23:21:55: EPOCH 11 - PROGRESS: at 39.39% examples, 76127 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:56: EPOCH 11 - PROGRESS: at 88.13% examples, 83886 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:56: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:56: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:56: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:56: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:56: EPOCH - 11 : training on 523616 raw words (198397 effective words) took 2.3s, 85762 effective words/s\n",
      "INFO - 23:21:57: EPOCH 12 - PROGRESS: at 45.83% examples, 90179 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:58: EPOCH 12 - PROGRESS: at 88.13% examples, 85762 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:21:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:21:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:21:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:21:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:21:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:21:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:21:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:21:58: EPOCH - 12 : training on 523616 raw words (198611 effective words) took 2.3s, 87538 effective words/s\n",
      "INFO - 23:21:59: EPOCH 13 - PROGRESS: at 45.83% examples, 88274 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:00: EPOCH 13 - PROGRESS: at 93.83% examples, 91244 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:00: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:00: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:00: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:00: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:00: EPOCH - 13 : training on 523616 raw words (198436 effective words) took 2.2s, 92106 effective words/s\n",
      "INFO - 23:22:01: EPOCH 14 - PROGRESS: at 41.54% examples, 78996 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:02: EPOCH 14 - PROGRESS: at 86.15% examples, 82668 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:03: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:03: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:03: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:03: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:03: EPOCH - 14 : training on 523616 raw words (198510 effective words) took 2.4s, 83890 effective words/s\n",
      "INFO - 23:22:04: EPOCH 15 - PROGRESS: at 41.54% examples, 75786 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:05: EPOCH 15 - PROGRESS: at 86.15% examples, 80363 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:05: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:05: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:05: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:05: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:05: EPOCH - 15 : training on 523616 raw words (198581 effective words) took 2.4s, 81945 effective words/s\n",
      "INFO - 23:22:06: EPOCH 16 - PROGRESS: at 43.69% examples, 84623 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:07: EPOCH 16 - PROGRESS: at 82.30% examples, 80365 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:08: EPOCH - 16 : training on 523616 raw words (198206 effective words) took 2.4s, 81957 effective words/s\n",
      "INFO - 23:22:09: EPOCH 17 - PROGRESS: at 37.41% examples, 75177 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:22:10: EPOCH 17 - PROGRESS: at 84.22% examples, 81083 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:10: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:10: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:10: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:10: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:10: EPOCH - 17 : training on 523616 raw words (199002 effective words) took 2.4s, 82760 effective words/s\n",
      "INFO - 23:22:11: EPOCH 18 - PROGRESS: at 37.41% examples, 73671 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:12: EPOCH 18 - PROGRESS: at 80.41% examples, 78630 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:13: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:13: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:13: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:13: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:13: EPOCH - 18 : training on 523616 raw words (198396 effective words) took 2.4s, 81342 effective words/s\n",
      "INFO - 23:22:14: EPOCH 19 - PROGRESS: at 39.39% examples, 76198 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:15: EPOCH 19 - PROGRESS: at 84.22% examples, 81552 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:15: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:15: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:15: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:15: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:15: EPOCH - 19 : training on 523616 raw words (198423 effective words) took 2.4s, 83913 effective words/s\n",
      "INFO - 23:22:16: EPOCH 20 - PROGRESS: at 45.83% examples, 87427 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:17: EPOCH 20 - PROGRESS: at 86.15% examples, 82977 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:17: worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:22:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:17: EPOCH - 20 : training on 523616 raw words (198555 effective words) took 2.3s, 84704 effective words/s\n",
      "INFO - 23:22:18: EPOCH 21 - PROGRESS: at 45.83% examples, 87339 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:19: EPOCH 21 - PROGRESS: at 90.04% examples, 86202 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:20: EPOCH - 21 : training on 523616 raw words (198767 effective words) took 2.3s, 87282 effective words/s\n",
      "INFO - 23:22:21: EPOCH 22 - PROGRESS: at 37.41% examples, 73080 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:22: EPOCH 22 - PROGRESS: at 82.30% examples, 80213 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:22: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:22: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:22: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:22: EPOCH - 22 : training on 523616 raw words (198578 effective words) took 2.4s, 81930 effective words/s\n",
      "INFO - 23:22:23: EPOCH 23 - PROGRESS: at 37.41% examples, 73061 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:24: EPOCH 23 - PROGRESS: at 84.22% examples, 81400 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:24: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:24: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:24: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:24: EPOCH - 23 : training on 523616 raw words (198256 effective words) took 2.4s, 83350 effective words/s\n",
      "INFO - 23:22:25: EPOCH 24 - PROGRESS: at 43.69% examples, 84952 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:26: EPOCH 24 - PROGRESS: at 86.15% examples, 82781 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:27: EPOCH - 24 : training on 523616 raw words (198874 effective words) took 2.3s, 84700 effective words/s\n",
      "INFO - 23:22:28: EPOCH 25 - PROGRESS: at 45.83% examples, 88156 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:29: EPOCH 25 - PROGRESS: at 90.04% examples, 87336 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:29: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:29: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:29: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:29: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:29: EPOCH - 25 : training on 523616 raw words (198773 effective words) took 2.2s, 89168 effective words/s\n",
      "INFO - 23:22:30: EPOCH 26 - PROGRESS: at 39.39% examples, 78449 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:31: EPOCH 26 - PROGRESS: at 86.15% examples, 83880 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:31: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:31: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:31: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:31: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:31: EPOCH - 26 : training on 523616 raw words (198588 effective words) took 2.3s, 84554 effective words/s\n",
      "INFO - 23:22:32: EPOCH 27 - PROGRESS: at 43.69% examples, 84364 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:33: EPOCH 27 - PROGRESS: at 86.15% examples, 83033 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:34: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:34: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:34: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:34: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:34: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:34: EPOCH - 27 : training on 523616 raw words (198845 effective words) took 2.4s, 84475 effective words/s\n",
      "INFO - 23:22:35: EPOCH 28 - PROGRESS: at 43.69% examples, 86535 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:36: EPOCH 28 - PROGRESS: at 91.99% examples, 90050 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:36: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:36: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:36: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:36: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:36: EPOCH - 28 : training on 523616 raw words (198267 effective words) took 2.3s, 86456 effective words/s\n",
      "INFO - 23:22:37: EPOCH 29 - PROGRESS: at 47.89% examples, 88959 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:38: EPOCH 29 - PROGRESS: at 95.70% examples, 90445 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:38: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:22:38: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:38: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:38: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:38: EPOCH - 29 : training on 523616 raw words (198682 effective words) took 2.2s, 91864 effective words/s\n",
      "INFO - 23:22:39: EPOCH 30 - PROGRESS: at 41.54% examples, 78875 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 23:22:40: EPOCH 30 - PROGRESS: at 86.15% examples, 81257 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 23:22:41: worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:22:41: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:22:41: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:22:41: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:22:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:22:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:22:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:22:41: EPOCH - 30 : training on 523616 raw words (198402 effective words) took 2.4s, 82518 effective words/s\n",
      "INFO - 23:22:41: training on a 15708480 raw words (5956686 effective words) took 70.2s, 84888 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 1.17 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48e12768512b82c2d5cf6a543e3a9f2515699a22"
   },
   "source": [
    "As we do not plan to train the model any further, we are calling init_sims(), which will make the model much more memory-efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "34dd51c7f2f39d016b982ef81e4df576f6b31bcb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:23:28: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a420d5a98eb860cff1f4bbac8cbe2054459b6200"
   },
   "source": [
    "# Exploring the model\n",
    "## Most similar to:\n",
    "\n",
    "Here, we will ask our model to find the word most similar to some of the most iconic characters of the Simpsons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8f3cfd8ac88978a4df31c90afa194bd6fa4f3f5"
   },
   "source": [
    "<img src=\"https://vignette.wikia.nocookie.net/simpsons/images/0/02/Homer_Simpson_2006.png/revision/latest?cb=20091207194310\" alt=\"drawing\" width=\"130\"/>\n",
    "\n",
    "Let's see what we get for the show's main character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "339207a733a1ac42fe60e32a29f9e5d5ca0a9275"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sweetheart', 0.7894335389137268),\n",
       " ('rude', 0.7790795564651489),\n",
       " ('embarrassing', 0.7584638595581055),\n",
       " ('crummy', 0.7582669258117676),\n",
       " ('marge', 0.7534692287445068),\n",
       " ('gee', 0.7454472780227661),\n",
       " ('hammock', 0.7417734861373901),\n",
       " ('sorry', 0.7301708459854126),\n",
       " ('duh', 0.7235903739929199),\n",
       " ('nice', 0.7174040675163269)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b6686e6fa956a98450259b063b4cf51019a6d0b"
   },
   "source": [
    "_A small precision here:_<br>\n",
    "The dataset is the Simpsons' lines of dialogue; therefore, when we look at the most similar words from \"homer\" we do **not** necessary get his family members, personality traits, or even his most quotable words. No, we get what other characters (as Homer does not often refers to himself at the 3rd person) said along with \"homer\", such as how he feels or looks (\"depressed\"), where he is (\"hammock\"), or with whom (\"marge\").\n",
    "\n",
    "Let's see what the bigram \"homer_simpson\" gives us by comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "23e5149b19f18f4f2f456d4c72afc5c188bcfba4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('montgomery_burns', 0.787061870098114),\n",
       " ('council', 0.7842810153961182),\n",
       " ('select', 0.7842451333999634),\n",
       " ('easily', 0.7823249101638794),\n",
       " ('pleased', 0.7806616425514221),\n",
       " ('governor', 0.7795811295509338),\n",
       " ('waylon', 0.7763530015945435),\n",
       " ('current', 0.7755867838859558),\n",
       " ('springfielder', 0.7745376229286194),\n",
       " ('robert', 0.7727807760238647)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer_simpson\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e3e121e022f2f659cf97bba42cecd3f3c9afb01"
   },
   "source": [
    "<img src=\"https://vignette.wikia.nocookie.net/simpsons/images/0/0b/Marge_Simpson.png/revision/latest?cb=20180626055729\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "What about Marge now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "22595f98c675a9697243b7e826b2840e5fc3e5f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ralphie', 0.7646403312683105),\n",
       " ('grownup', 0.7591749429702759),\n",
       " ('homer', 0.7534692287445068),\n",
       " ('worry', 0.750848650932312),\n",
       " ('rude', 0.7488858103752136),\n",
       " ('rapture', 0.7459008097648621),\n",
       " ('sure', 0.7443293929100037),\n",
       " ('sweetheart', 0.7424054145812988),\n",
       " ('crummy', 0.7367110252380371),\n",
       " ('marriage', 0.7363215684890747)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"marge\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b4857ff1159695c72c22417cf52dc84e0dfc9ea"
   },
   "source": [
    "<img src=\"https://vignette.wikia.nocookie.net/simpsons/images/6/65/Bart_Simpson.png/revision/latest?cb=20180319061933\" alt=\"drawing\" width=\"100\"/>\n",
    "\n",
    "Let's check Bart now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "ac9ba47738e596dce6552099e76f303f28577943"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.8597580790519714),\n",
       " ('hearing', 0.8372896909713745),\n",
       " ('convince', 0.8071838617324829),\n",
       " ('impress', 0.7873097658157349),\n",
       " ('mom', 0.7846276760101318),\n",
       " ('strangle', 0.7837260961532593),\n",
       " ('jealous', 0.780017614364624),\n",
       " ('homework', 0.7789014577865601),\n",
       " ('upset', 0.7785576581954956),\n",
       " ('surprised', 0.7771762609481812)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9afcf59e71f797f8e4d6d4b4ce39e359b19a450"
   },
   "source": [
    "Looks like it is making sense!\n",
    "\n",
    "<img src=\"https://vignette.wikia.nocookie.net/simpsons/images/9/9d/Groundskeeper_Willie.png/revision/latest?cb=20130424154035\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "Willie the groundskeeper for the last one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8b5937dfd7584f168a33060c435036cad5b390b"
   },
   "source": [
    "## Similarities:\n",
    "Here, we will see how similar are two words to each other :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "367755f5c9e00de4bc5056c978f5b50a38c1368b"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity(\"moe_'s\", 'tavern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d31383aa2f6310a38ec671f2ca4b0fcb195551dd"
   },
   "source": [
    "Who could forget Moe's tavern? Not Barney.\n",
    "\n",
    "<img src=\"https://vignette.wikia.nocookie.net/simpsons/images/6/6c/MaggieSimpson.PNG/revision/latest?cb=20180314210204\" alt=\"drawing\" width=\"100\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "349828078b5a438d93e5494478e88095913dc58e"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('maggie', 'baby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d7842e5b5cab743db56ff120d75ad3974c429d8"
   },
   "source": [
    "Maggie is indeed the most renown baby in the Simpsons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ee5e2532214b20fef0a597bc5ad355762fcc281"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('bart', 'nelson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08a999d758ac687d626b631a8ce393eaa26f41e7"
   },
   "source": [
    "Bart and Nelson, though friends, are not that close, makes sense!\n",
    "\n",
    "## Odd-One-Out:\n",
    "\n",
    "Here, we ask our model to give us the word that does not belong to the list!\n",
    "\n",
    "Between Jimbo, Milhouse, and Kearney, who is the one who is not a bully?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d982e44d9c212b5ee09bcaebd050a725ab5e508e"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match(['jimbo', 'milhouse', 'kearney'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12d3c035c89718e70c193f095b46e38dccef6b0d"
   },
   "source": [
    "Milhouse of course!\n",
    "<img src=\"https://vignette.wikia.nocookie.net/simpsons/images/9/91/Milhouse_Van_Houten_2.png/revision/latest?cb=20180429212659\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "What if we compared the friendship between Nelson, Bart, and Milhouse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cafd4a7bec6d6255ea3f5f06df951546c0d783a9"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match([\"nelson\", \"bart\", \"milhouse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01c0148dc758db74ed8078ca54e8393ada090c8c"
   },
   "source": [
    "Seems like Nelson is the odd one here!\n",
    "\n",
    "<img src=\"https://vignette.wikia.nocookie.net/simpsons/images/4/40/Picture0003.jpg/revision/latest?cb=20110623042517\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "Last but not least, how is the relationship between Homer and his two sister-in-laws?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "445912f7d89b3cb1550926be161d134e6689f54f"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match(['homer', 'patty', 'selma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df95cdff693e843ab4b4c174fea24029447573cd"
   },
   "source": [
    "Damn, they really do not like you Homer!\n",
    "\n",
    "## Analogy difference:\n",
    "Which word is to woman as homer is to marge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "812961e79dde9f2032f708755ca287c0aef838d0"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"marge\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a4ff4a8c8c582c6d9c704a042e8cc5d18b7bd6c"
   },
   "source": [
    "\"man\" comes at the first position, that looks about right!\n",
    "\n",
    "Which word is to woman as bart is to man?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cfef57b94b635abb58a4ff191785506c78ec9d9"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"bart\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef520bd7dd974d14afcb8e69266579ac0b703714"
   },
   "source": [
    "Lisa is Bart's sister, her male counterpart!\n",
    "<img src=\"https://vignette.wikia.nocookie.net/simpsons/images/5/57/Lisa_Simpson2.png/revision/latest?cb=20180319000458\" alt=\"drawing\" width=\"100\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "773c0acc8750ba8e728ff261f2e9ec39694c245c"
   },
   "source": [
    "### t-SNE visualizations:\n",
    "t-SNE is a non-linear dimensionality reduction algorithm that attempts to represent high-dimensional data and the underlying relationships between vectors in a lower-dimensional space.<br>\n",
    "Here is a good tutorial on it: https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27ec46110042fc28da900b1b344ae4e0692d5dc2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22693eaa25253b38cee3c5cd5db6b6fdddb575a4"
   },
   "source": [
    "Our goal in this section is to plot our 300 dimensions vectors into 2 dimensional graphs, and see if we can spot interesting patterns.<br>\n",
    "For that we are going to use t-SNE implementation from scikit-learn.\n",
    "\n",
    "To make the visualizations more relevant, we will look at the relationships between a query word (in <font color='red'>**red**</font>), its most similar words in the model (in <font color=\"blue\">**blue**</font>), and other words from the vocabulary (in <font color='green'>**green**</font>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "489a7d160dcd92da0ce42a3b5b461368c9ffe5f1"
   },
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, 300), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=50).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3943c170a5f5f09974d90c89bdb9ec761e63a416"
   },
   "source": [
    "Code inspired by: [[2]](#References:)\n",
    "\n",
    "## 10 Most similar words vs. 8 Random words:\n",
    "Let's compare where the vector representation of Homer, his 10 most similar words from the model, as well as 8 random ones, lies in a 2D graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18d788b2a92f94771a5f9485a885d44dfba62a94"
   },
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, 'homer', ['dog', 'bird', 'ah', 'maude', 'bob', 'mel', 'apu', 'duff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c73fc2faaf0baecc84f02a97b50cb9ccefa48686"
   },
   "source": [
    "Interestingly, the 10 most similar words to Homer ends up around him, so does Apu and (sideshow) Bob, two recurrent characters.\n",
    "\n",
    "## 10 Most similar words vs. 10 Most dissimilar\n",
    "\n",
    "This time, let's compare where the vector representation of Maggie and her 10 most similar words from the model lies compare to the vector representation of the 10 most dissimilar words to Maggie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10c77b072f7c281f2be919341be116565c20d8a8"
   },
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, 'maggie', [i[0] for i in w2v_model.wv.most_similar(negative=[\"maggie\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87315bfbaceb3733bd7af035db6c59cfc4b1ba7f"
   },
   "source": [
    "Neat! Maggie and her most similar words form a distinctive cluster from the most dissimilar words, it is a really encouraging plot!\n",
    "\n",
    "## 10 Most similar words vs. 11th to 20th Most similar words:\n",
    "\n",
    "Finally, we are going to plot the most similar words to Mr. Burns ranked 1st to 10th versus the ones ranked 11th to 20th:\n",
    "\n",
    "(PS: Mr. Burns became mr_burn after the preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6f0bc598922f4f2cd17d2511560242a3c35fdd9"
   },
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, \"mr_burn\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"mr_burn\"], topn=20)][10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "011aa35b717d985f1d9fb820208531222055ff7b"
   },
   "source": [
    "As we can see, and that is very nice, all the 20 words are forming one cluster around Mr. Burns.\n",
    "\n",
    "# Final Thoughts\n",
    "\n",
    "I hope you found this tutorial useful and had as much fun reading it as I had writing it. Please do not hesitate to leave any comments, questions or suggestions you might have. See you around!\n",
    "\n",
    "Also, please check [Supportiv](http://www.supportiv.com) around! (Simpson-ized logo)\n",
    "\n",
    "<img src=\"https://fontmeme.com/permalink/180904/cc3d27a8aaa88189e764ee9d02331d0d.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "# Materials for more in depths understanding:\n",
    "* Word Embeddings introduction: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "* Word2Vec introduction: https://skymind.ai/wiki/word2vec\n",
    "* Another Word2Vec introduction: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "* A great Gensim implentation tutorial: http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W467ScBjM2x\n",
    "* Original articles from Mikolov et al.: https://arxiv.org/abs/1301.3781 and https://arxiv.org/abs/1310.4546\n",
    "\n",
    "\n",
    "# Acknowledgements\n",
    "\n",
    "* [Pouria Mojabi](https://www.linkedin.com/in/pouria-mojabi-1873615/), co-fouder of Supportiv Inc.\n",
    "\n",
    "# References:\n",
    "* [1]. Neural Net picture: McCormick, C. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://www.mccormickml.com\n",
    "* [2]. Aneesha Bakharia Medium article: https://medium.com/@aneesha/using-tsne-to-plot-a-subset-of-similar-words-from-word2vec-bb8eeaea6229\n",
    "\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Knowledge_Graph",
   "language": "python",
   "name": "knowledge_graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
